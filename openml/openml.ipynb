{"cells":[{"cell_type":"code","execution_count":null,"id":"163b38c3","metadata":{"id":"163b38c3"},"outputs":[],"source":["import numpy as np\n","import gzip\n","import shutil\n","import random\n","import os\n","import numpy as np\n","import vowpalwabbit as vw\n","import gc\n","import re\n","import argparse\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"id":"9a382a10","metadata":{"id":"9a382a10"},"outputs":[],"source":["def read_data(filename):\n","    if filename.endswith('.vw.gz'):\n","        filename = uncompress(filename)\n","    with open(filename,'r') as f:\n","        print(f'opening {filename}')\n","        lines = f.readlines()\n","        lines = [line.rstrip() for line in lines]\n","    return lines\n","\n","def uncompress(filename):\n","    names = filename.split('.')\n","    vw_name = names[0] + '.' + names[1]\n","    with gzip.open(filename, 'rb') as f_in:\n","        with open(vw_name, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","    return vw_name\n","\n","def process_actions(sup_data):\n","    actions = []\n","    for data in sup_data:\n","        action = data.split('|')[0]\n","        actions.append(action)\n","    action_list = list(set(actions))\n","    action_size = len(action_list)\n","    print('actions: ', action_list)\n","\n","    act_cnt =[0]*action_size\n","    for a in actions:\n","        act_cnt[action_list.index(a)] += 1\n","    assert(len(actions)==np.sum(act_cnt))\n","    best_const_act_acc = (max(act_cnt)/1.0*len(actions))\n","    print('action count:', act_cnt)\n","\n","    action_maj = np.zeros(action_size)\n","    if np.max(act_cnt) / np.sum(act_cnt) > 0.5:\n","        action_maj[np.argmax(act_cnt)] = 1\n","\n","    return action_size, action_list, act_cnt, action_maj, best_const_act_acc\n","\n","def create_train_test_split(data):\n","    sup_data_len = len(data)\n","    random.shuffle(data)\n","    train_data = data[:int(0.9 * sup_data_len)]\n","    test_data = data[int(0.9 * sup_data_len):]\n","    return train_data, test_data"]},{"cell_type":"code","execution_count":null,"id":"2cc80f7b","metadata":{"id":"2cc80f7b"},"outputs":[],"source":["\n","# from utils import *\n","\n","class IGL_xCI():\n","    def __init__(self, did=554, epochs=10,verbose=False,amlt=True) -> None:\n","        self.epochs = epochs\n","        if amlt:\n","            self.data_dir = '/mnt/data/openml/'\n","        else:\n","#             self.data_dir = os.getcwd()\n","#             self.data_dir += '/data/openml/'\n","            self.data_dir = './data/'\n","        self.verbose = verbose\n","        self.unbalanced = 0\n","\n","        vw_file_prefix = 'ds_'+str(did)+'_'\n","        for f in os.listdir(self.data_dir):\n","            if f.startswith(vw_file_prefix):\n","                vw_file_name = f\n","                break\n","        \n","        try:\n","            self.data = read_data(self.data_dir+vw_file_name)\n","            self.action_size, self.action_list, self.action_count, self.action_maj, best_const_action_acc = process_actions(self.data)\n","            self.const_act_acc = best_const_action_acc\n","            if sum(self.action_maj)>0:\n","                self.unbalanced =1\n","            self.train_data, self.test_data = create_train_test_split(self.data)\n","            self.action_flip_list = np.zeros(self.action_size)\n","            self.num_samples = len(self.data)\n","            train_samples = len(self.train_data)\n","\n","            print(f'total epochs: {self.epochs}\\n')\n","        except:\n","            print('Error in reading dataset!')\n","            exit(1)\n","\n","        self.dummi_f = []\n","        self.dummi_psi = []\n","\n","        for a_idx in range(self.action_size):\n","            vw_f = vw.Workspace(f'--max_prediction 1 --min_prediction -1', \\\n","                                quiet=False, enable_logging=True)\n","            self.dummi_f.append(vw_f)\n","            \n","            vw_psi = vw.Workspace(f'--min_prediction -1 --max_prediction 1', quiet=True)\n","            self.dummi_psi.append(vw_psi)\n","\n","\n","    def collect_data(self, sup_data):\n","        data = [[] for _ in range(self.action_size)]\n","        for idx in range(len(sup_data)):\n","            x = sup_data[idx].split('|')[1]\n","            x_label = sup_data[idx].split('|')[0]\n","            a_idx = np.random.randint(self.action_size)\n","            rwd = int(int(self.action_list[a_idx]) == int(x_label))     \n","            y = str(1 + a_idx)\n","            if rwd:\n","                y += ' ' + str(1 + self.action_size)  \n","            data[a_idx].append([x,y])\n","        return data\n","\n","\n","    def train(self, igl_data, sup_data):\n","        for epoch in range(self.epochs):\n","            if self.verbose:\n","                print('\\nepoch: ', epoch)\n","            for a_idx in range(self.action_size):\n","                \n","                a_data = igl_data[a_idx]\n","                dummi_f_a = self.dummi_f[a_idx]\n","                dummi_psi_a = self.dummi_psi[a_idx]\n","            \n","                for n,sample in enumerate(a_data):\n","                    x,y = sample\n","                    x_ind, y_ind = random.choice(a_data)\n","                    f_x = dummi_f_a.predict(' | ' + x)\n","                    psi_y = dummi_psi_a.predict(' | ' + y)\n","                    f_x_ind = dummi_f_a.predict(' | ' + x_ind)\n","                    psi_y_ind = dummi_psi_a.predict(' | ' + y_ind)\n","                    \n","                    # VW reduction for the first term of contrastive loss\n","                    # sample for (y,f(' | ' + x))\n","                    psi_eg_1 = str(f_x) + ' | ' + y\n","                    # sample for (x,psi(' | ' + y))\n","                    f_eg_1 = str(psi_y) + ' | ' + x\n","                    \n","                    # VW reduction for the second term of contrastive loss\n","                    # sample for (y,1) if f(x) < psi(y_ind)\n","                    imp_psi_eg_2 = ' '  + str(np.fabs(psi_y_ind - f_x))\n","                    if psi_y_ind < f_x:\n","                        psi_eg_2 = str(-1) + imp_psi_eg_2\n","                    elif psi_y_ind > f_x:\n","                        psi_eg_2 = str(1) + imp_psi_eg_2\n","                    else:\n","                        r = np.random.choice([1,-1])\n","                        psi_eg_2 = str(r)\n","                    psi_eg_2 +=  ' | ' + y_ind\n","                    # sample for (x,1) if f(x) > psi(y_ind)\n","                    imp_f_eg_2 = ' ' + str(np.fabs(psi_y - f_x_ind))\n","                    if f_x_ind < psi_y:\n","                        f_eg_2 = str(-1) + imp_f_eg_2\n","                    elif f_x_ind > psi_y:\n","                        f_eg_2 = str(1) + imp_f_eg_2\n","                    else:\n","                        r = np.random.choice([1,-1])\n","                        f_eg_2 = str(r)\n","                    f_eg_2 += ' | ' + x_ind\n","\n","                    # in the first half of epochs, we learn reward decoder dummi_psi_a using the contrastive loss\n","                    # in the second half of epochs, we use the decoded reward from dummi_psi_a to label data, and then use DM approach in CB to learn the expected reward\n","                    # note that, we also use the value of dummi_f_a from the first half of epochs as the initialization of the learned expected reward\n","                    # train f and psi per example\n","                    dummi_f_a.learn(f_eg_1)\n","                    if epoch < self.epochs / 2:\n","                        dummi_f_a.learn(f_eg_2)\n","                    \n","                        dummi_psi_a.learn(psi_eg_1)\n","                        dummi_psi_a.learn(psi_eg_2)  \n","                \n","                if epoch < self.epochs / 2:\n","                    action_sum = 0\n","                    action_rwd = 0\n","                    for n,sample in enumerate(a_data):\n","                        x,y = sample\n","                        psi_y = dummi_psi_a.predict(' | ' + y)\n","                        action_sum +=1\n","                        action_rwd +=psi_y\n","\n","                    if self.action_maj[a_idx]:\n","                        if action_rwd/action_sum<0:\n","                            self.action_flip_list[a_idx] = 1\n","                    else:\n","                        if action_rwd/action_sum>0:\n","                            self.action_flip_list[a_idx] = 1\n","            \n","            # per_action_acc.append(accuracy)\n","            if self.verbose:\n","                accuracy = self.evaluate(sup_data)\n","                print('f',accuracy)\n","\n","        # return per_action_acc\n","\n","\n","    def f_prediction(self, x):\n","        a_score = []\n","        for a_idx in range(self.action_size):\n","            if self.action_flip_list[a_idx]:\n","                a_score.append(0 - self.dummi_f[a_idx].predict(' | ' + x))\n","            else:\n","                a_score.append(self.dummi_f[a_idx].predict(' | ' + x))\n","        return self.action_list[np.argmax(a_score)]\n","\n","\n","\n","    def evaluate(self, sup_data):\n","        total_num = 0\n","        total_rwd = 0\n","        \n","        action_picked_dict = {}\n","        act_cnt = [0] * self.action_size\n","        \n","        for idx in range(len(sup_data)):\n","            x = ' | ' + sup_data[idx].split('|')[1]\n","            x_label = sup_data[idx].split('|')[0]\n","            predict_label = self.f_prediction(x)\n","            rwd = int(int(predict_label) == int(x_label))\n","            total_num += 1\n","            total_rwd += rwd\n","            \n","            if predict_label not in action_picked_dict:\n","                action_picked_dict[predict_label] = 1\n","            else:\n","                action_picked_dict[predict_label] += 1\n","                \n","        print('chosen actions: ', action_picked_dict)\n","        return(total_rwd / total_num)\n","\n","    def run(self):\n","        igl_data = self.collect_data(self.train_data)\n","        self.train(igl_data, self.test_data)\n","        accuracy = self.evaluate(self.test_data)\n","        return accuracy"]},{"cell_type":"code","execution_count":null,"id":"8382430f","metadata":{"id":"8382430f"},"outputs":[],"source":["def main():\n","\n","    agent = IGL_xCI(did=116,verbose=True,amlt=False,epochs=10)\n","    # agent = IGL_xCI(did=1568,verbose=True,amlt=False,epochs=10)\n","    # agent = IGL_xCI(did=554,verbose=True,amlt=False,epochs=10) # MNIST\n","    \n","    igl_data = agent.collect_data(agent.train_data)\n","    accuracy = agent.evaluate(agent.test_data)\n","    print('f: ',accuracy)\n","\n","    agent.train(igl_data, agent.test_data)\n","    accuracy = agent.evaluate(agent.test_data)\n","    print('f: ', accuracy)"]},{"cell_type":"code","execution_count":null,"id":"94461521","metadata":{"id":"94461521","scrolled":false},"outputs":[],"source":["main()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"neurips.ipynb","provenance":[{"file_id":"1oY3sysHkXRxYsmzzU0Yd3cND5lM7cuIq","timestamp":1653537194479},{"file_id":"1MRXjo4UzLYVwkZNZI4L7w_seG7fx5ykq","timestamp":1652477210557}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}
